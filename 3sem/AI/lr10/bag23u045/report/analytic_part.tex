\chapter{Аналитическая часть}

Метод "мешка слов" широко используется в обработке текстов и позволяет представить текстовый документ в виде набора количественных признаков, 
где каждый признак соответствует частоте или наличию слова в документе. 
Однако, с увеличением числа документов и уникальных слов (признаков) размерность векторов значительно возрастает, что может замедлять вычисления и усложнять анализ.

Для оптимизации анализа и снижения размерности векторов используются различные статистические параметры, 
такие как дисперсия, математическое ожидание, ковариация, корреляции Пирсона и Спирмена.

\section{Математическое ожидание}
Математическое ожидание (среднее значение) показывает центральную тенденцию распределения значений признака. 
Оно указывает на усредненное значение, вокруг которого располагаются все значения признака.

Для признака математическое ожидание определяется как:

\begin{math}
    \label{mean}
    Mean = \frac{1}{N}\sum_{j}^{N}x_{ij}
\end{math} \cite{lib:mean}

\section{Дисперсия}
Дисперсия показывает, насколько каждый признак (его частота) в наборе данных отклоняется от своего среднего значения.
Высокая дисперсия указывает на значительное разбросанное распределение значений вокруг среднего, тогда как низкая — на более сгруппированные значения.

Для признака дисперсия определяется как:

\begin{math}
    \label{var}
    Var = \frac{1}{N}\sum_{j = 1}^{N}(x_{ij}-\overline{x}_{i})
\end{math} \cite{lib:var}

\section{Ковариация}
Ковариация измеряет, как два признака изменяются вместе. 
Положительная ковариация означает, что признаки увеличиваются или уменьшаются одновременно, тогда как отрицательная указывает на противоположные направления.

Для признаков ковариация определяется как:

\begin{math}
    \label{cov}
    Cov(x_{i},x_{j})= \frac{1}{N}\sum_{k = 1}^{N}(x_{i,k}-\overline{x}_{i})(x_{j,k}-\overline{x}_{j})
\end{math} \cite{lib:covariance}

\section{Корреляция Пирсона}
Корреляция Пирсона измеряет линейную зависимость между двумя признаками. 
Значение корреляции варьируется от -1 до 1, где 1 означает полную положительную зависимость, -1 — полную отрицательную, а 0 — отсутствие линейной зависимости.

Для признаков корреляция Пирсона определяется как:

\begin{math}
    \label{pearson}
    Corr_{Pearson} = \frac{Cov(x_{i},x_{j})}{\sigma_{i}\sigma_{j}}
\end{math} \cite{lib:pearson}

\section{Корреляция Спирмена}
Корреляция Спирмена оценивает монотонную зависимость между признаками. 
В отличие от корреляции Пирсона, она подходит для оценки нелинейных зависимостей.

Для признаков корреляция Спирмена определяется как:

\begin{math}
    \label{spearman}
    Corr_{Spearman} = 1 - \frac{6\sum_{k=1}^{N}(R_{x_{i},k}-R_{x_{j},k})^{2}}{N(N^{2}-1)}
\end{math} \cite{lib:spearman}

\section*{Вывод}
Каждый из рассмотренных способов уменьшения количества параметров имеет свои ограничения, но, в целом, 
их использование предполагает выделение дублирующих или неинформативных признаков.

\clearpage

