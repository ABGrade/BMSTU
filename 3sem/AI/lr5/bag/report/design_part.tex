\chapter{Конструкторская часть}

Разработанная программа предназначена для классификации изображений рукописных цифр из набора данных MNIST. 
Её работа основана на использовании нейронных сетей, где качество классификации исследуется в зависимости от архитектуры сети и соотношения обучающей и тестовой выборок.

\section{Принцип действия программы}

Программа состоит из нескольких последовательных этапов. 
На первом этапе данные предобрабатываются: 
изображения нормализуются путём деления значений интенсивности пикселей на 255, что приводит их в диапазон $[0, 1]$. 
Метки классов преобразуются в формат one-hot encoding, который представляет каждый класс в виде вектора, 
где значение $1$ соответствует истинному классу, а остальные элементы равны $0$.

Далее строится архитектура нейронной сети. 
На вход подаются изображения размером $28 \times 28$ с одним каналом, которые сначала преобразуются в плоский вектор с помощью слоя Flatten. 
Затем к сети добавляются скрытые слои, состоящие из 128 нейронов каждый. 

В качестве функции активации используется ReLU (см. \ref{ReLU}).
ReLU минимизирует проблему исчезающих градиентов, что особенно важно при обучении глубоких сетей. 
Выходной слой сети имеет 10 нейронов с функцией активации softmax, 
что позволяет интерпретировать выход модели как вероятности принадлежности к каждому из десяти классов:

\begin{math}
    P(y_i \mid x) = \frac{\exp(z_i)}{\sum_{j=1}^{10} \exp(z_j)}
\end{math} \cite{lib:softmax}
,где $z_i$ — значение на выходе $i$-го нейрона.

Обучение модели осуществляется с использованием функции потерь KL-дивергенции (см. \ref{KLDivergence})

\section{Результаты и их интерпретация}

Для исследования влияния архитектуры сети и размера обучающей выборки были проведены исследования с различным числом скрытых слоёв (0, 1, 5) 
и соотношением обучающей и тестовой выборок (от $10\%-90\%$ до $90\%-10\%$). 
В процессе обучения фиксировались значения точности и потерь на обучающей, валидационной и тестовой выборках.

Также была проведена аналитическая оценка минимально необходимого размера обучающей выборки на основе неравенства Чебышёва:
\begin{math}
    n \geq \frac{\sigma^2}{\epsilon^2 \delta}
\end{math} \cite{lib:chebyshev}

\section*{Вывод}

Разработана программа для классификации изображений из набора MNIST с использованием нейронных сетей. 
Реализована возможность исследования влияния количества скрытых слоёв и соотношения обучающей и тестовой выборок на качество классификации. 
Программа использует ReLU-активацию, функцию потерь KL-дивергенции и метод оптимизации Adam. 
Также предусмотрен аналитический расчёт минимального размера обучающей выборки по неравенству Чебышёва. 
Итоги создают основу для дальнейших исследований и анализа.

\clearpage
